#!/bin/bash
# full_pipeline.sh - Menggunakan semua 7 tools

DOMAIN=$1
if [ -z "$DOMAIN" ]; then
    echo "Usage: $0 <domain>"
    exit 1
fi

echo "üöÄ FULL BUG HUNTING PIPELINE - 7 TOOLS INTEGRATION"
echo "=================================================="
echo "Target: $DOMAIN"
echo ""

# 1. SUBFINDER - Subdomain enumeration
echo "[1/7] üîç Subfinder: Finding subdomains..."
./tools/subfinder -d $DOMAIN -silent -o targets/recon/subfinder_$DOMAIN.txt

# 2. ASSETFINDER - Additional subdomains
echo "[2/7] üîç Assetfinder: Additional assets..."
./tools/assetfinder --subs-only $DOMAIN > targets/recon/assetfinder_$DOMAIN.txt

# Merge results
cat targets/recon/*_$DOMAIN.txt | sort -u > targets/recon/all_subs.txt
SUBS_COUNT=$(wc -l < targets/recon/all_subs.txt)
echo "‚úÖ Found $SUBS_COUNT unique subdomains"

# 3. HTTPX - Find live hosts
echo "[3/7] üåê HTTPX: Finding live hosts..."
./tools/httpx -l targets/recon/all_subs.txt -silent -threads 20 \
    -ports 80,443,8080,8443,3000 -o targets/recon/live_hosts.txt
LIVE_COUNT=$(wc -l < targets/recon/live_hosts.txt)
echo "‚úÖ Found $LIVE_COUNT live hosts"

# 4. KATANA - Crawling
echo "[4/7] üï∑Ô∏è Katana: Crawling URLs..."
cat targets/recon/live_hosts.txt | ./tools/katana -d 2 -jc -kf -c 10 \
    -o targets/scan/crawled_urls.txt
URLS_COUNT=$(wc -l < targets/scan/crawled_urls.txt 2>/dev/null || echo 0)
echo "‚úÖ Crawled $URLS_COUNT URLs"

# 5. NUCLEI - Vulnerability scanning
echo "[5/7] üéØ Nuclei: Vulnerability scanning..."
if [ $LIVE_COUNT -gt 0 ]; then
    ./tools/nuclei -l targets/recon/live_hosts.txt \
        -severity medium,high,critical \
        -etags intrusive \
        -rate-limit 30 \
        -c 10 \
        -o targets/results/nuclei_$DOMAIN.txt
    NUCLEI_COUNT=$(wc -l < targets/results/nuclei_$DOMAIN.txt 2>/dev/null || echo 0)
    echo "‚úÖ Found $NUCLEI_COUNT potential vulnerabilities"
fi

# 6. DALFOX - XSS scanning
echo "[6/7] ‚ú® Dalfox: XSS scanning..."
if [ -f "targets/scan/crawled_urls.txt" ] && [ $URLS_COUNT -gt 0 ]; then
    # Extract URLs with parameters
    grep "?" targets/scan/crawled_urls.txt > targets/scan/param_urls.txt
    PARAM_COUNT=$(wc -l < targets/scan/param_urls.txt 2>/dev/null || echo 0)
    
    if [ $PARAM_COUNT -gt 0 ]; then
        ./tools/dalfox file targets/scan/param_urls.txt \
            --skip-bav \
            --only-custom-payload \
            -o targets/results/xss_$DOMAIN.txt
        XSS_COUNT=$(wc -l < targets/results/xss_$DOMAIN.txt 2>/dev/null || echo 0)
        echo "‚úÖ Checked $PARAM_COUNT parameter URLs, found $XSS_COUNT XSS issues"
    else
        echo "‚ö†Ô∏è  No parameter URLs found for XSS testing"
    fi
fi

# 7. FFUF - Directory fuzzing (limited)
echo "[7/7] üîé FFUF: Quick directory fuzzing..."
if [ -f "targets/recon/live_hosts.txt" ] && [ $LIVE_COUNT -gt 0 ]; then
    head -3 targets/recon/live_hosts.txt | while read url; do
        echo "   Scanning: $url"
        ./tools/ffuf -u $url/FUZZ \
            -w wordlists/common.txt \
            -t 10 \
            -rate 20 \
            -timeout 3 \
            -mc 200,301,302 \
            -o targets/results/ffuf_$(echo $url | sed 's/[^a-zA-Z0-9]/_/g').json \
            -quiet 2>/dev/null
    done
    echo "‚úÖ Directory fuzzing completed for top 3 hosts"
fi

echo ""
echo "=================================================="
echo "üéâ PIPELINE COMPLETED!"
echo "üìä Results:"
echo "   Subdomains: $SUBS_COUNT"
echo "   Live hosts: $LIVE_COUNT"
echo "   Crawled URLs: $URLS_COUNT"
[ -n "$NUCLEI_COUNT" ] && echo "   Nuclei findings: $NUCLEI_COUNT"
[ -n "$XSS_COUNT" ] && echo "   XSS findings: $XSS_COUNT"
echo ""
echo "üìÅ Check folders:"
echo "   targets/recon/    - Reconnaissance data"
echo "   targets/scan/     - Crawled data"
echo "   targets/results/  - Vulnerability findings"
echo "=================================================="
